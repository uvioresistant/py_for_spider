# 1.爬虫概述：爬虫就是获取网页并提取和保存信息的自动化程序

# 1>获取网页：首先要做的就是获取网页的源代码。源代码里包含了网页的部分有用信息，只要把源代码获取下来，就可以从中提取想要的信息了
# 向网站的服务器发送一个请求，返回的响应便是网页源代码，关键部分就是构造一个请求并发送给服务器，然后接收到响应并解析出来
# python提供了许多库来实现这个操作，如urllib、requests。可用这些库来实现HTTP请求操作，请求和响应都可以用类库提供的数据结构表示
# 得到响应后，只需解析数据结构中的Body部分即可，即得到网页的源代码，即用程序来实现获取网页的过程

# 2>提取信息：分析网页源代码，从中提取想要的数据。
# 最常用的方法是采用正则表达式提取，这是一个万能的方法，但在构造正则表达式时比较复杂且容易出错
# 网页的结构有一定的规则，还有一些根据网页节点属性、CSS选择器、Xpath来提取网页信息的库，如Beautiful Soup、pyquery、lxml
# 使用库可以高效快速地从中提取网页信息，如节点的属性、文本值，提取信息是爬虫非常重要的部分，可使杂乱的数据变得条理清晰，以便
# 后续处理和分析数据

# 3>保存数据：提取数据后，一般会将提取到的数据保存到某处以便后续使用，保存形式多种多样，可简单的保存为TXT文本或JSON文本，
#               也可保存到数据库MySQL和MongoDB，也可保存至远程服务器，如借助SFTP进行操作

# 4>自动化程序：爬虫代替人来完成这份爬取工作的自动化程序，可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效运行

# 2.能抓怎样的数据：网页中最常见的便是常规网页，对应着HTML代码，最常抓取的便是HTML源代码
# 有些网页返回的不是HTML代码，而是一个JSON字符串(API接口大多采用这样的形式),这种格式方便传输和解析，同样可抓取，数据提取更方便
# 还可以利用爬虫，将各种二进制数据，如图片、音频、视频抓取下来，然后保存成对应的文件名
# 还有各种拓展名的文件，如CSS、JS和配置文件，只要在浏览器中可以访问，就可将其抓取下来
# 对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取

# 3.JS渲染页面：用urllib或requests抓取页面时，得到的源代码实际和浏览器中看到的不一样
# 现在网页越来越多采用Ajax前端模块化工具来构建，整个网页可能都是有JS渲染出来的，也就是说元素的HTML代码就是个空壳，如：
<!DOCTYPE html>
<html>
  <head>
    <title>2.2.2_网页结构.html</title>
	    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  </head>
  <body>
   <div id="container">     # body节点里只有一个id为container的节点，
   </div>
  </body>
  <script src="app.js"></script>    # 但body节点后引入了app.js，负责整个网站的渲染
</html>
# 浏览器中打开这个页面时，首先会加载这个HTML内容，接着浏览器会发现其中引入了一个app.js文件，然后会去请求这个文件，获取该文件后，
# 会执行其中的JS代码，JS会改变HTML中的节点，向其添加内容，最后得到完整的页面。
# 但用urllib或requests等库请求当前页面时，得到的只是这个HTML代码，不会继续加载这个JS文件，也就看不到浏览器中的内容了
# 对于这种情况，可以分析后台Ajax接口，也可使用Selenium、Splash库来实现模拟JS渲染

